# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/5. Text to semantic token modeling.ipynb.

# %% auto 0
__all__ = ['load_datasets', 'TSARTransformer', 'make_model']

# %% ../nbs/5. Text to semantic token modeling.ipynb 1
import torch
import torch.nn as nn
from torch.profiler import record_function

# %% ../nbs/5. Text to semantic token modeling.ipynb 2
from pathlib import Path
import pylab as plt
import pandas as pd

# %% ../nbs/5. Text to semantic token modeling.ipynb 3
import whisper
from spear_tts_pytorch.train import *
from spear_tts_pytorch.modules import *

# %% ../nbs/5. Text to semantic token modeling.ipynb 11
def load_data(path):
    data = pd.DataFrame(dict(stoks=[str(x) for x in Path(path).rglob('*.stoks')]))
    data['text'] = data['stoks'].apply(lambda x: Path(x).with_suffix('.txt').read_text())
    return data

# %% ../nbs/5. Text to semantic token modeling.ipynb 14
import torch.nn.functional as F

class SADataset(torch.utils.data.Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.data)
            
    def __repr__(self):
        return f"<Dataset: {len(self)} samples>"
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        Stoks = torch.load(row['stoks'], map_location='cpu')[0,:,0]
        Ttoks = self.tokenizer.encode(row['text'])
        return F.pad(torch.tensor(Ttoks), (0, 200 - len(Ttoks)), value=self.tokenizer.eot).to(torch.long), F.pad(Stoks, (0, 1500 - len(Stoks)), value=1024).to(torch.long)

# %% ../nbs/5. Text to semantic token modeling.ipynb 19
def load_datasets(path):
    tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)
    data = load_data(path)
    
    val_data, train_data = data[:300], data[300:]

    return SADataset(train_data, tokenizer), SADataset(val_data, tokenizer)

# %% ../nbs/5. Text to semantic token modeling.ipynb 21
class TSARTransformer(nn.Module):
    def __init__(self, width=384, depth=6, n_head=6):
        super(TSARTransformer, self).__init__()

        self.encoder = Encoder(length=200, codes=50364, width=width, n_head=n_head, depth=depth)
        self.decoder = Decoder(length=1500, codes=1024, width=width, n_head=n_head, depth=depth)

    def forward(self, Ttoks, Stoks, loss=True):
        with record_function("encoder"):
            xenc = self.encoder(Ttoks.to(torch.long))
        with record_function("decoder"):
            logits = self.decoder(Stoks, xenc)
        if loss is not None:
            with record_function("loss"):
                loss = F.cross_entropy(logits.reshape(-1,logits.shape[-1]), Stoks.view(-1))
        return logits, loss

# %% ../nbs/5. Text to semantic token modeling.ipynb 22
def make_model(size):
    if size == 'micro':
        return TSARTransformer(depth=3)
    elif size == 'tiny':
        return TSARTransformer(depth=4)
