{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71174a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb61c7c",
   "metadata": {},
   "source": [
    "# Perform Voice Activity Detection (VAD)\n",
    "\n",
    "We want to start with voice activity detection to make sure we are not cutting off words and sentences in the middle.\n",
    "This should improve transcription reliability and make both the quantization and T2S model training easier.\n",
    "\n",
    "**Usage:**  \n",
    "```\n",
    "python -m whisperspeech.vad https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-large-wo6454-flac-000002.tar\n",
    "```\n",
    "\n",
    "You can pass in either a URL or a local file name. The result will go into a file in the current directory named after the source file but replacing `flac` with `vad` (check the `flac_to_vad_name` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffbdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from pathlib import Path\n",
    "from fastprogress import progress_bar\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "import whisperx\n",
    "import random\n",
    "import numpy as np\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# some of the original file names have a dot in their name\n",
    "# webdataset does not like it so let's patch it\n",
    "def fix_dots_in_names(name):\n",
    "    name, ext = name.rsplit('.', 1)\n",
    "    return \".\".join((name.replace('.', '_'), ext))\n",
    "\n",
    "def load_dataset(url, decode=True):\n",
    "    ds = wds.WebDataset(url, rename_files=fix_dots_in_names)\n",
    "    if not decode: return ds\n",
    "    return ds.decode(wds.torch_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714048d3",
   "metadata": {},
   "source": [
    "We use the voice activity detection model from WhisperX (but we don't use their merging algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def extract_segments(vad_result, max_duration):\n",
    "    binarize = whisperx.vad.Binarize(max_duration=max_duration)\n",
    "    segments = binarize(vad_result)\n",
    "    return [(x.start, x.end) for x in segments.get_timeline()]\n",
    "\n",
    "def segment_audio(vad_model, audio):\n",
    "    vad_result = vad_model({\"waveform\": audio, \"sample_rate\": 16000})\n",
    "    return extract_segments(vad_result, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f2559",
   "metadata": {},
   "source": [
    "Test just a few files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='10' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10/10 00:10&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 7.5K Sep 21 08:51 librilight-large-wo6454-vad-000002.tar.gz\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_03_molesworth_64kb.vad.npy\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_04_molesworth_64kb.vad.npy\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_08_molesworth_64kb.vad.npy\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_09_molesworth_64kb.vad.npy\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_10_molesworth_64kb.vad.npy\n",
      "large/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_11_molesworth_64kb.vad.npy\n",
      "large/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_13_dickinson_64kb.vad.npy\n",
      "large/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_30_dickinson_64kb.vad.npy\n",
      "large/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_16_pyle_64kb.vad.npy\n",
      "large/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_25_pyle_64kb.vad.npy\n"
     ]
    }
   ],
   "source": [
    "# test it locally\n",
    "input:str  = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-large-wo6454-flac-000002.tar'\n",
    "output:str = input.rsplit(\"/\", 1)[1].replace('flac', 'vad') + \".gz\"\n",
    "\n",
    "ds = load_dataset(input)\n",
    "vad_model = whisperx.vad.load_vad_model('cuda')\n",
    "\n",
    "with wds.TarWriter(output) as sink:\n",
    "    for s in progress_bar(ds, total=10):\n",
    "        audio, sr = s['flac']\n",
    "        assert(sr == 16000)\n",
    "        sink.write({\n",
    "            \"__key__\": s['__key__'],\n",
    "            \"vad.npy\": np.array(segment_audio(vad_model, audio), dtype=np.float16)\n",
    "        })\n",
    "        \n",
    "!ls -lh {output}\n",
    "!tar tf {output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110f217",
   "metadata": {},
   "source": [
    "## Batch processing\n",
    "\n",
    "Let's put everything above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def flac_to_vad_name(input):\n",
    "    return input.rsplit(\"/\", 1)[1].replace('flac', 'vad') + \".gz\"\n",
    "\n",
    "@call_parse\n",
    "def process_shard(\n",
    "    input:str,       # input shard URL/path\n",
    "    output:str=None  # output shard URL/path\n",
    "):\n",
    "    if output is None: output = flac_to_vad_name(input)\n",
    "    \n",
    "    ds = torch.utils.data.DataLoader(load_dataset(input), num_workers=2, batch_size=None)\n",
    "    vad_model = whisperx.vad.load_vad_model('cuda')\n",
    "    \n",
    "    tmp = output+\".tmp\"\n",
    "    with wds.TarWriter(tmp) as sink:\n",
    "        for s in progress_bar(ds, total='noinfer'):\n",
    "            audio, sr = s['flac']\n",
    "            assert(sr == 16000)\n",
    "            sink.write({\n",
    "                \"__key__\": s['__key__'],\n",
    "                \"vad.npy\": np.array(segment_audio(vad_model, audio), dtype=np.float16)\n",
    "            })\n",
    "    os.rename(tmp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='335' class='' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [335/335 03:30&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for reference, this was the performance on a single 4090:\n",
    "process_shard('https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
