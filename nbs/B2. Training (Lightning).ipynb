{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afd7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e79ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfd417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from fastprogress import progress_bar, master_bar\n",
    "import fastprogress\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "import IPython\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.profiler import record_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232153f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import lightning.pytorch as pl\n",
    "import math\n",
    "\n",
    "class TrainingTask(pl.LightningModule):\n",
    "    def __init__(self, model, model_hparams=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model_hparams = model_hparams\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Initialize AdamW optimizer\"\"\"\n",
    "        all_params = set(self.model.parameters())\n",
    "        wd_params = set()\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                wd_params.add(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    wd_params.add(m.bias)\n",
    "        no_wd_params = all_params - wd_params\n",
    "\n",
    "        optimizer = torch.optim.AdamW(lr=self.model_hparams['lr0'], betas=(0.9, 0.95), fused=True,\n",
    "            params=[\n",
    "                {\"params\": list(wd_params), \"weight_decay\": self.model_hparams['weight_decay']},\n",
    "                {\"params\": list(no_wd_params), \"weight_decay\": 0.0},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # modified from https://github.com/Lightning-AI/lightning/issues/5449#issuecomment-1501597319\n",
    "        def num_steps_per_epoch() -> int:\n",
    "            \"\"\"Get number of steps\"\"\"\n",
    "            # Accessing _data_source is flaky and might break\n",
    "            dataset = self.trainer.fit_loop._data_source.dataloader()\n",
    "            dataset_size = len(dataset)\n",
    "            num_devices = max(1, self.trainer.num_devices)\n",
    "            # math.ceil so always overestimate (underestimating throws exceptions)\n",
    "            num_steps = math.ceil(dataset_size / (self.trainer.accumulate_grad_batches * num_devices))\n",
    "            return num_steps\n",
    "        \n",
    "        if self.model_hparams['pct_start'] is None:\n",
    "            # 10k updates by default\n",
    "            total_steps = self.model_hparams['epochs'] * num_steps_per_epoch()\n",
    "            self.model_hparams['pct_start'] = min(0.3, 10000 / total_steps)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            pct_start=self.model_hparams['pct_start'],\n",
    "            max_lr=self.model_hparams['lr0'],\n",
    "            steps_per_epoch=num_steps_per_epoch(),\n",
    "            epochs=self.model_hparams['epochs']\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        train_logits, train_loss = self.model.forward(x, y)\n",
    "\n",
    "        self.log(\"train_loss\", train_loss, sync_dist=True)\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        val_logits, val_loss = self.model.forward(x, y)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss, sync_dist=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        test_logits, test_loss = self.model.forward(x, y)\n",
    "\n",
    "        self.log(\"test_loss\", test_loss, sync_dist=True)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5dd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', type=str, help='Task to train')\n",
    "parser.add_argument('--seed', type=int, default=0, help='Global training seed')\n",
    "parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\n",
    "parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n",
    "parser.add_argument('--input-dir', type=str, default='', help='input data path') # fixed in the model for now\n",
    "parser.add_argument(\"--checkpoint-dir\", type=str, default=\"./checkpoints/\", help=\"directory to save the checkpoints\")\n",
    "parser.add_argument('--epochs', type=int, default=10, help='total training epochs')\n",
    "parser.add_argument('--weight-decay', type=float, default=1e-2, help='optimizer weight decay')\n",
    "parser.add_argument('--lr0', type=float, default=1e-4, help='optimizer initial learning rate')\n",
    "parser.add_argument('--pct-start', type=float, default=None, help='optimizer percentage of total number of epochs when learning rate rises during one cycle (defaults to 10k updates)')\n",
    "parser.add_argument('--model-size', type=str, default='small', help='model size')\n",
    "\n",
    "args = parser.parse_args().__dict__\n",
    "\n",
    "task_name: str = args.pop(\"task\")\n",
    "input_dir: str = args.pop(\"input_dir\")\n",
    "model_size: str = args.pop(\"model_size\")\n",
    "checkpoint_dir: str = args.pop(\"checkpoint_dir\")\n",
    "num_workers: int = args.pop(\"workers\")\n",
    "batch_size: int = args.pop(\"batch_size\")\n",
    "epochs: int = args.pop(\"epochs\")\n",
    "\n",
    "hyp_params = {}\n",
    "hyp_params['pct_start'] = args['pct_start']\n",
    "hyp_params['weight_decay'] = args['weight_decay']\n",
    "hyp_params['lr0'] = args['lr0']\n",
    "hyp_params['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import importlib\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "wandb_logger = WandbLogger(project=f\"SpearTTS-{task_name}\")\n",
    "\n",
    "ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "     dirpath=f'{task_name}-{epochs}e',\n",
    "     filename=task_name+\"-{epoch}-{step}-{val_loss:.2f}\",\n",
    "     monitor=\"val_loss\",\n",
    "     save_top_k=4,\n",
    "     every_n_epochs=1,\n",
    " )\n",
    "\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "task = importlib.import_module(\"spear_tts_pytorch.\"+task_name)\n",
    "\n",
    "train_ds, val_ds = task.load_datasets(input_dir)\n",
    "\n",
    "val_loader = DataLoader(val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "model = task.make_model(model_size) \n",
    "\n",
    "task = TrainingTask(model, model_hparams=hyp_params)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=hyp_params['epochs'],\n",
    "                  accelerator=\"gpu\",\n",
    "                  profiler=\"simple\",\n",
    "                  precision='16-mixed',\n",
    "                  enable_checkpointing=True,\n",
    "                  logger=wandb_logger,\n",
    "                  callbacks=[ckpt_callback, lr_monitor_callback])\n",
    "\n",
    "trainer.fit(model=task, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00406652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffffe92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
